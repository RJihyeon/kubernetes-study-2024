# 쿠버네티스 스케줄링

**스케줄링이란**

- 컨테이너나 가상 머신과 같은 인스턴스를 새로 생성할 때, 그 인스턴스를 어느 서버에 생성할 것인지를 결정하는 일
- 클라우드 자원의 효율적인 사용과 고가용성 확보

![image](https://github.com/user-attachments/assets/acf8c7a6-6fe3-41cd-80c4-4b0c4ba021d4)

**왜 중요할까?**

1. **특정한 자원 최적화**
    
    예를들어, 빠른 파일 입출력을 위해 SSD가 필요한 경우 SSD를 가진 워커 노드에 파드를 배치!
    
2. **부하분산**
    
    워크 노드 전체에 최대한 골고루 파드를 배치해서 서버가 과부하되는 걸 막을 수 있음!
    
3. **고가용성 보장**
    
    서버 장애가 발생해도 앱이 무중단 운영 가능~
    

스케줄링 과정을 간략하게 살펴보고, 파드를 생성하기 위한 YAML 파일에서 사용할 수 있는 여러 스케줄링 설정값에 대해 알아보자





### 1. 파드가 실제로 노드에서 생성되기까지의 과정
- kubectl이나 API 서버로 파드 생성 요청을 전송하면 일어나는 일

```
💡
1. ServiceAccount, RoleBinding 등의 기능을 이용해 파드 생성 요청한 사용자의 인증 및 인가 작업을 수행
2. ResourceQuota, LimitRange와 같은 어드미션 컨트롤러가 해당 파드 요청을 적절히 변형(Mutate)하거나 검증(Validate)함
3. 어드미션 컨트롤러의 검증을 통과해 최종적으로 파드 생성이 승인 됐다면, k8s는 해당 파드를 워커 노드 중 한곳에 생성
```

스케줄링은 3단계에서 발생! 

**쿠버네티스의 컴포넌트들**
- 쿠버네티스는 기본적으로 여러 핵심 컴포넌트가 실행되며 `kube-system` 네임스페이스에서 관리된다.
- 그 중에서도 스케줄링에 관여하는 컴포넌트는 다음 2가지
    - **`kube-scheduler`**: 파드의 스케줄링 담당.
    - **`etcd`**: 쿠버네티스의 상태 데이터를 저장하는 분산 키-값 데이터베이스.
    - +) `kube-apiserver` : kubectl로 상호 통신할 수 있는 API 서버 컴포넌트
- 얘네 또한 파드로서 실행되기 때문에 `kubectl get pods` 로 쉽게 확인 가능

```bash
$ kubectl get pods -n kube-system
NAME                                  READY   STATUS    RESTARTS   AGE
etcd-ip-10-43-0-20.ap-northeast-2.compute.internal   1/1     Running   27         21d
kube-apiserver-ip-10-43-0-20.ap-northeast-2.compute.internal   1/1     Running   3          17h # 클로스터와의 통신을위한 서버
kube-scheduler-ip-10-43-0-20.ap-northeast-2.compute.internal   1/1     Running   27         21d
```


**etcd**
- 분산 코디네이터(Distributed Coordinator)라고 불리는 도구의 일종
- 여러 컴포넌트가 정상적으로 상호 작용할 수 있도록 데이터를 조정하는 역할을 담당
- 쿠버네티스 또한 클러스터 운용에 필요한 정보 (현재 생성된 디플로이먼트, 파드 목록 정보, 클러스터 자체 정보)를 etcd에 저장

**etcd 접근방식**
- 무조건 API 서버를 통해서만 접근 가능!
  ![image](https://github.com/user-attachments/assets/782252dc-bc9e-4a0e-91a1-2422b4d986f4)
    - ex) `kubectl get pods` 실행 → API 서버에 요청 전달 →  etcd 데이터 읽어와서 → 반환

NodeName항목 (etcd에 저장된)
- 파드가 어느 워커 노드에서 실행되는지 확인 가능 !
```bash
$ kubectl get pods mypod -o yaml | grep -F3 nodeName
nodeName: ip-10-43-0-30.ap-northeast-2.compute.internal
```

**스케줄링 과정**
1. **API 서버**: 파드 생성 요청 -> etcd에 저장. 
    이때! nodename은 비어있음.
2. **kube-scheduler**: **`nodeName`**이 비어 있는 파드 감지 -> 적합한 노드 선택 -> **`nodeName`** 설정.
3. **kubelet**: Watch를 통해, API 서버로부터 **`nodeName`**이 설정된 파드 정보를 감지하고 해당 노드에서 파드 실행.


```💡
💡💡복기)  `kubelet`은 쿠버네티스 클러스터의 각 워커 노드에서 독립적으로 실행되는 데몬
```






### 2. 파드가 생성될 노드를 선택하는 스케줄링 과정

파드를 어떤 노드에서 실행할 지 무슨 기준으로 정할까?

**스케줄링의 핵심 단계**
스케줄링 과정에서 파드를 실행할 적합한 노드를 선택하는 단계는 크게 두 가지로 나뉜다.
1. 노드 필터링
2. 노드 스코어링


**1. 노드 필터링 (Filtering)**
파드를 실행할 수 없는 노드를 걸러내는 단계
- 설정된 CPU, 메모리 Requests 값보다 자원이 부족한 노드.
- 노드가 장애 상태 (`kubectl get nodes`의 STATUS가 `Ready`가 아닌 경우).
- 마스터 노드(기본적으로 워크로드를 실행하지 않음).

**2. 노드 스코어링 (Scoring)**
필터링 단계에서 남은 후보 노드에 점수를 매겨 최적의 노드를 선택
- 파드의 컨테이너 이미지가 이미 로컬에 존재하면 점수 증가.
- 노드의 자원 여유가 많을수록 점수 증가.
최고득점 노드가 파드를 실행하게 된당!!!

노드 스코어링은 쿠버네티스에 내장된 로직에 의해 계산되기 때문에 알고리즘 수정 경우가 많지 않음!
따라서 스케줄링 조건을 YAML 파일에 설정해서 노드 필터링 단계에서 적용할 수 있도록 구성하는 것이 일반적이다.
노드 필터링 단계에서 사용할 수 있는 방법들을 살펴보자.


### 3. NodeSelector와 Node Affinity, Pod Affinity
✅이번 장의 예제들은 별도 언급이 없는 한  **1개의 마스터와 3개의 워커노드**로 구성된 클러스터 기준으로 설명

```bash
$ kubectl get nodes
NAME               STATUS   ROLES                  AGE   VERSION
ip-172-16-0-10     Ready    control-plane,master   21m   v1.23.6
ip-172-16-0-30     Ready    <none>                 91m   v1.23.6
ip-172-16-0-32     Ready    <none>                 91m   v1.23.6
```


**1. nodeName과 nodeSelector를 사용한 스케줄링 방법**
- nodeName : 특정 워커 노드에 파드 배치하는 가장 확실한 방법!
- YAML 파일을 작성하여 **`nodeName`**을 명시
**`nodename-nginx.yaml`**:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  nodeName: ip-10-43-0-30.ap-northeast-2.compute.internal
  containers:
  - name: nginx
    image: nginx:latest
```
```bash
$ kubectl apply -f nodename-nginx.yaml
$ kubectl get pods -o wide
NAME    READY   STATUS    RESTARTS   AGE   IP           NODE
nginx   1/1     Running   0          61s   192.168.1.55   ip-10-43-0-30...
```

- **주의사항**: 고정된 노드 이름을 사용하므로 노드가 다운되거나 환경 변하면 YAML파일 수정해야됨. 유연성 떨어짐
- **보완**: 라벨(Label) 기반 배치가 권장됨!
    - 노드에 라벨 설정해서 특정 조건에 맞는 노드로 파드를 배치하는 방법
    - 더 유연한 방법임

**라벨을 사용한 노드 선택의 예시**
일단은 라벨을 추가해보자.
![image](https://github.com/user-attachments/assets/0f200d08-fcc4-4708-be40-ebb96437cbb5)
```bash
# 노드 라벨 확인하기
$ kubectl get nodes --show-labels
NAME                                STATUS   AGE   VERSION   LABELS
ip-172-43-0-20.ap-northeast-2...    Ready    6h3m  v1.23.6   beta.kubernetes.io/arch=amd64,
                                                              beta.kubernetes.io/instance-type=t2.medium,
                                                              beta.kubernetes.io/os=linux, ...

# 노드에 라벨 추가하기 (mylabel/disk=ssd)
$ kubectl label nodes ip-10-43-0-30.ap-northeast-2.compute.internal mylabel/disk=ssd
node/ip-10-43-0-30.ap-northeast-2.compute.internal labeled

# 노드에 라벨 추가하기(mylabel/disk=hdd)
$ kubectl label nodes ip-10-43-0-31.ap-northeast-2.compute.internal mylabel/disk=hdd
$ kubectl label nodes ip-10-43-0-32.ap-northeast-2.compute.internal mylabel/disk=hdd
node/ip-10-43-0-31.ap-northeast-2.compute.internal labeled
node/ip-10-43-0-32.ap-northeast-2.compute.internal labeled

# 노드에 추가된 라벨 확인
kubectl get nodes --show-labels
NAME                           STATUS   ROLES    AGE   VERSION   LABELS
ip-10-43-0-30.ap-northeast-2   Ready    <none>   63m   v1.23.6   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=t2.medium,beta.kubernetes.io/os=linux,kubernetes.io/hostname=ip-10-43-0-30,mylabel/disk=ssd
ip-10-43-0-31.ap-northeast-2   Ready    <none>   63m   v1.23.6   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=t2.medium,beta.kubernetes.io/os=linux,kubernetes.io/hostname=ip-10-43-0-31,mylabel/disk=hdd
ip-10-43-0-32.ap-northeast-2   Ready    <none>   63m   v1.23.6   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=t2.medium,beta.kubernetes.io/os=linux,kubernetes.io/hostname=ip-10-43-0-32,mylabel/disk=hdd

# 노드 라벨 제거
$ kubectl label nodes ip-10-43-0-32.ap-northeast-2.compute.internal mylabel/disk-
```

**nodeSelector** 
- 특정 라벨이 설정된 노드에 파드를 배치!
- nodeSelector는 해당 라벨이 존재하는 노드가 여러 개일 경우 그 중 하나를 선택하기 때문에 노드의 이름에 종속적이지 않게 YAML파일을 작성할 수 있음.
- Pod 배포 YAML 파일 **`nodeselector-nginx.yaml`**
    - 단일 파드를 배포할 때 사용

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-nodeselector
spec:
  nodeSelector: # 이부분! 
    mylabel/disk: hdd
  containers:
  - name: nginx
    image: nginx:latest
```
- Deployment 배포 YAML 파일 **`deployment-nginx-nodeselector.yaml`**
    - 아래 예시로 3개의 파드가 생성되고 mylabel/disk=hdd 라벨이 있는 노드에만 배치됨
    - 라벨 조건을 만족하는 노드가 여러개라면 각 파드는 다른 노드에 분산될 수 있음 ! 즉, 무조건 파드들이 다 동일한 노드에 배치되는 거 아님

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: deployment-nginx
  template:
    metadata:
      labels:
        app: deployment-nginx
    spec:
      nodeSelector: # 이부분! 
        mylabel/disk: hdd
      containers:
      - name: nginx
        image: nginx:latest

```


**2. Node Affinity를 이용한 스케줄링 방법** 

nodeSelector: 단순히 라벨의 키-값이 같은지만 비교해 노드 선택하므로 활용 방법이 다양하진 않음. 

**NodeSelector**의 기능을 확장하여, 노드 선택 조건을 **Hard**(필수 조건) 또는 **Soft**(선호 조건)로 설정할 수 있음
1. **requiredDuringSchedulingIgnoredDuringExecution**: 반드시 충족해야 하는 조건
2. **preferredDuringSchedulingIgnoredDuringExecution**: 선호하는 조건(충족되지 않아도 배치 가능)

예시를 살펴보자
- **Hard 조건을 사용한 YAML 예시** **`nodeaffinity-required.yaml`**
    - 조건을 충족하지 않는 노드에는 절대 파드가 배치되지 않음.
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-nodeaffinity-required
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: mylabel/disk ## 라벨을 기준으로 조건을 설정.
            operator: In ## ssd 또는 hdd 값 중 하나를 가진 노드에 파드 배치.
            values:
            - ssd
            - hdd
  containers:
  - name: nginx
    image: nginx:latest
```

- **Soft 조건을 사용한 YAML 예시 `nodeaffinity-preferred.yaml`**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-nodeaffinity-preferred
spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 80 #조건의 중요도를 설정 (숫자가 클수록 우선 적용)
        preference: # ssd 라벨이 있는 노드를 선호하지만, 조건을 만족하지 않아도 다른 노드에 배치 가능
          matchExpressions:
          - key: mylabel/disk
            operator: In 
            values:
            - ssd
  containers:
  - name: nginx
    image: nginx:latest
```

```bash
# 위 YAML 파일 적용시켜주기
$ kubectl apply -f nodeaffinity-required.yaml
$ kubectl apply -f nodeaffinity-preferred.yaml
pod/nginx-nodeaffinity-required created
pod/nginx-nodeaffinity-preferred created

# 배포된 파드 상태 확인
$ kubectl get pods -o wide
NAME                              READY   STATUS    RESTARTS   AGE   IP           NODE
nginx-nodeaffinity-required       1/1     Running   0          30s   192.168.1.5  ip-10-43-0-30
nginx-nodeaffinity-preferred      1/1     Running   0          25s   192.168.1.6  ip-10-43-0-31
```




**3. Pod Affinity를 이용한 스케줄링 방법** 

Pod Affinity는 특정 조건을 만족하는 다른 파드가 실행 중인 노드에 파드를 배치하는 방법! Node Affinity와는 다르게, 파드 간의 상호 관계를 기반으로 스케줄링을 수행
- 사용방법은 node affinity와 거의 같음!

  **`podaffinity-required.yaml`**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-podaffinity
spec:
  affinity:
    podAffinity: #특정 조건을 만족하는 파드가 있는 노드에 새 파드를 배치
      requiredDuringSchedulingIgnoredDuringExecution: # 설정된 조건을 반드시 충족해야만 스케줄링
        labelSelector: #mylabel/database=mysql 라벨이 설정된 파드와 동일한 노드로 스케줄링
          matchExpressions:
          - key: mylabel/database
            operator: In
            values:
            - mysql
        topologyKey: failure-domain.beta.kubernetes.io/zone
        #요 topologyKey값이 동일한 노드들 중에서 스케줄링을 수행
        #특정 Zone 내에서 파드를 배치하도록 설정
  containers:
  - name: nginx
    image: nginx:latest
```

요 YAML 파일의 경우에는 아래 사진에서 노드 A혹은 노드 B에 할당될 수 있다!

![image](https://github.com/user-attachments/assets/d82ef2ac-cff8-44c5-ace3-e9ddf430d194)
```bash
$ kubectl apply -f podaffinity-required.yaml
pod/nginx-podaffinity created

$ kubectl get pods -o wide
NAME                READY   STATUS    RESTARTS   AGE   IP           NODE
nginx-podaffinity   1/1     Running   0          30s   192.168.1.7  ip-10-43-0-32
```

- 2개 파드를 동일한 AZ or Region의 노드에 할당해야하는 경우 활용하기 좋음

호스트 이름을 기반으로 Pod Affinity를 설정한다면????

**`podaffinity-hostname-topology.yaml`**

```yaml
 podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        labelSelector:
          matchExpressions:
          - key: mylabel/database
            operator: In
            values:
            - mysql
        topologyKey: kubernetes.io/hostname #요부분!
        #kubernetes.io/hostname은 기본적으로 쿠버네티스가 모든 노드에 설정하는 라벨로, 노드의 호스트 이름을 나타냄
```

- 기본적으로 모든 노드의 호스트 이름은 고유하기 때문에 하나의 토폴로지에 두 개 이상의 노드가 존재할 수 없어짐!!!
- 하나의 노드가 하나의 토폴로지에 대응된다는 의미
- matchExpressoin을 반드시 만족하는 파드가 위치한 노드를 선택하게될 것
![image](https://github.com/user-attachments/assets/ef7d881c-9e09-4adc-ad41-2239660207e1)




**4. Pod Anti-affinity를 이용한 스케줄링 방법**
![image](https://github.com/user-attachments/assets/247a59be-6438-4c16-b19d-7f939f680f06)
- Pod Affinity와 반대로 동작!
    - 특정 파드가 **같은 토폴로지(예: Zone, Region)** 내의 노드에 배치되지 않도록 설정하여 **고가용성(HA)**과 **리소스 분산**을 보장하는 스케줄링 방식
- **`podAffinity`**를 **`podAntiAffinity`**로 변경하면 적용됨!
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod-antiaffinity
spec:
  affinity:
    podAntiAffinity: ## 요부분
      requiredDuringSchedulingIgnoredDuringExecution: #조건을 반드시 충족!!!
      ##무조건 아래의 애랑은 다른 토폴로지에 배치
        - labelSelector:
            matchExpressions:
              - key: mylabel/database
                operator: In
                values:
                  - mysql
          topologyKey: failure-domain.beta.kubernetes.io/zone
  containers:
    - name: nginx
      image: nginx:latest

```
```yaml
spec:
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution: ## 가급적 충족
      #가급적다른 토폴로지에 파드 배치하려고 하지만,, 동일한 토폴로지에 배치될 수도 
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
                - key: mylabel/database
                  operator: In
                  values:
                    - mysql
            topologyKey: failure-domain.beta.kubernetes.io/zone
          weight: 80

```

**각 노드에 하나의 파드만 1:1로 배치되도록 해서 독점적 배치 보장하기!** 
언제? DaemonSet처럼 노드마다 고유의 파드를 배치할 때 유용
- topology를 hostname으로 위에 언급했듯이 설정하면 됩니당(hostname은 무조건 고유하니까..)
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-nginx
  labels:
    app: deployment-nginx
spec:
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
                - key: app
                  operator: In
                  values:
                    - deployment-nginx
            topologyKey: kubernetes.io/hostname
          weight: 100
```
